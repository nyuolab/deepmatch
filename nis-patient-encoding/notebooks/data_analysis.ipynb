{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to identify the optimal data processing structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KEPT FEATURES\n",
    "\n",
    "| Variable | Description | Type of Feature | Range |\n",
    "| --- | --- | --- | --- |\n",
    "| AGE | Patient age | Continuous | [1, 255] |\n",
    "| AMONTH | Admission month | Categorical | [1, 12] |\n",
    "| AWEEKEND | Admission day is on a weekend | Categorical | [0, 1] |\n",
    "| BODYSYSTEMn | Chronic Condition Indicators - body system (2014) | Categorical | [1, 18] |\n",
    "| CHRONBn | Chronic Condition Indicators - body system (â‰¤2013) | Categorical | [1, 18] |\n",
    "| CHRONn | ICD-9-CM Chronic Condition Indicators | Categorical | [0, 1] |\n",
    "| CM_[comorbidity]| Presence of a comorbidity | Categorical | [0, 1] |\n",
    "| DXn| ICD-9-CM Diagnosis code | Categorical | [0, 299910] |\n",
    "| DXCCSn| Single-level CCS: ICD-9-CM Dx Class | Categorical | [1-259] |\n",
    "| DXMCCSn | Multi-Level CCS for ICD-9-CM Dx Class | Categorical | [0, 99999999] |\n",
    "| E_CCS1 | Single-level CCS for ICD-9-CM External Cause of Injury | Categorical | [2601, 2629] |\n",
    "| E_MCCSn | Multi-Level CCS for ICD-9-CM External Cause of Injury\t| Categorical | [0, 99999999] |\n",
    "| ECODEn | ICD-9-CM External Cause of Injury Code | Categorical | [0, 9991] |\n",
    "| ELECTIVE | Elective vs non-elective admission | Categorical | [0, 1] |\n",
    "| FEMALE | sex/gender | Categorical | [0, 1] |\n",
    "| HCUP_ED| HCUP indicator of emergency department record | Categorical | [0, 4] |\n",
    "| HOSP_DIVISION | Census Division of hospital (STRATA) | Categorical | [1, 9] |\n",
    "| NIS_STRATUM | Stratum used to post-stratify hospital | Categorical | [1011, 9433] |\n",
    "| ORPROC | Major operating room ICD-9-CM procedure indicator | Categorical | [0, 1] |\n",
    "| PAY1 | Expected primary payer | Categorical | [1, 6] |\n",
    "| PCLASSn | ICD-9-CM Procedure class | Categorical | [1, 4] |\n",
    "| PL_NCHSn | Category urban-rural classification scheme | Categorical | [1, 6] |\n",
    "| PRn| Procedure codes |  Categorical | [0, 9999] |\n",
    "| PRCCSn | CCS class for ICD-9-CM procedures | Categorical | [1, 231] |\n",
    "| PRDAYn| Number of days since admission to procedure n | Continuous? | [-4, 365] |\n",
    "| PRMCCS1 | Multi-Level CCS for ICD-9-CM Procedures\t| Categorical | [0, 999999] |\n",
    "| TRAN_IN | Indicator of transfer into the hospital | Categorical | [0, 1] |\n",
    "| ZIPINC_QRTL | Mean household income for zipcode | Categorical | [1, 4] | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMOVED FEATURES\n",
    "\n",
    "| Variable | Description | Reason for Exclusion |\n",
    "| --- | --- | --- |\n",
    "| AGE_NEONATE| Neonatal age | Ignore neonates for now |\n",
    "| DIED | Death during hospitalization stay | OUTCOME |\n",
    "| DISCWT | Weights | No useful information |\n",
    "| DISPUNIFORM | disposition | OUTCOME |\n",
    "| DQTR | discharge quarter | OUTCOME |\n",
    "| DRG | DRG in use on discharge date | OUTCOME |\n",
    "| DRG24 | DRG, Version 24 | OUTCOME |\n",
    "| DRGVER | DRG or MS-DRG grouper version used on discharge date | OUTCOME |\n",
    "| DRG_NoPOA | DRG in use on discharge date, calculated without POA | OUTCOME |\n",
    "| HOSP_NIS| Hospital ID | Probably extraneous for now, though would be interesting to keep...| \n",
    "| KEY_NIS | Admission ID | No useful information |\n",
    "| HOSPBRTH | HCUP indicator indicating in-hospital birth | Ignore neonates |\n",
    "| LOS | Length of stay | OUTCOME |\n",
    "| MDC | MDC (Major Diagnostic Category) in effect on discharge date\t| OUTCOME |\n",
    "| MDC24 | MDC (Major Diagnostic Category) version 24 | OUTCOME | \n",
    "| MDC_NoPOA | MDC (Major Diagnostic Category) in effect on discharge date, calc w/o POA | OUTCOME |\n",
    "| NCHRONIC | Number of chronic conditions | No useful information |\n",
    "| NDX | Number of ICD-9-CM diagnoses on this discharge | No useful information |\n",
    "| NECODE | Number of ICD-9-cM EoCoI Codes on this Record | No useful information |\n",
    "| NPR | Number of ICD-9-CM procedures on this discharge | No useful information |\n",
    "| NEOMAT | Neonatal or maternal ICD-9-CM DX / PR | Ignore neonates |\n",
    "| RACE | Race | For obvious reasons |\n",
    "| TOTCHG | Total charges, cleaned (continuous) | OUTCOME |\n",
    "| TRAN_OUT | Indicator of a transfer outside of the hospital | Categorical | [0, 1] |\n",
    "| YEAR | Year of hospitalization | No useful information |\n",
    "| APRDRG | All patient refined DRG, Categorical [0, 999] | Dependent feature class |\n",
    "| APRDRG_Risk_Mortality | All Patient Refined DRG: Risk of Mortality Subclass, Categorical [0, 4] | Dependent feature class |\n",
    "| APRDRG_Severity | All Patient Refined DRG: Severity of Illness Subclass, Categorical [0, 4] | Dependent feature class |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclusion Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's identify how many values are missing from our feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import re\n",
    "import tables\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data/raw/'\n",
    "MISS_VAL_FILL = -128\n",
    "YEARS = ['2012', '2013', '2014']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTCOMES = ['DIED', 'DISPUNIFORM', 'DRG', 'LOS', 'TOTCHG', 'TRAN_OUT', 'APRDRG', 'APRDRG_SEVERITY', 'APRDRG_RISK_MORTALITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['AGE', \n",
    "            'AMONTH', \n",
    "            'AWEEKEND', \n",
    "            'BODYSYSTEMn', \n",
    "            'CHRONBn',\n",
    "            'CHRONn',\n",
    "            'CM_AIDS',\n",
    "            'DXn',\n",
    "            'DXCCSn',\n",
    "            'DXMCCSn'\n",
    "            'E_CCS1',\n",
    "            'E_MCCSn',\n",
    "            'ECODEn',\n",
    "            'ELECTIVE',\n",
    "            'FEMALE',\n",
    "            'HCUP_ED',\n",
    "            'HOSP_DIVISION',\n",
    "            'NIS_STRATUM',\n",
    "            'ORPROC',\n",
    "            'PAY1',\n",
    "            'PCLASSn',\n",
    "            'PL_NCHSn',\n",
    "            'PRn',\n",
    "            'PRCCSn',\n",
    "            'PRDAYn',\n",
    "            'PRMCCSn',\n",
    "            'TRAN_IN',\n",
    "            'TRAN_OUT',\n",
    "            'ZIPINC_QRTL'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_feature(header):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if header in FEATURES:\n",
    "        return True\n",
    "        \n",
    "    elif header[:3] == 'CM_':\n",
    "        return True\n",
    "        \n",
    "    else:\n",
    "        prefix = re.split('\\d', header)[0] # Prefix before #\n",
    "        n_feature = prefix + 'n'\n",
    "        \n",
    "        if n_feature in FEATURES:\n",
    "            return n_feature\n",
    "        \n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(series):\n",
    "    num_missing = np.sum(series == -128)\n",
    "#     unique, counts = np.unique(series, return_counts=True)\n",
    "#     return {'missing': num_missing, 'unique': unique, 'counts': counts}\n",
    "    return {'missing': num_missing}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = h5py.File(DATA_FOLDER + 'NIS.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_FOLDER + 'NIS_columnstats_2012_2014.json', 'r') as f:\n",
    "    headers_found = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2012, Header PRCCS1 Completed\n",
      "Year 2012, Header PRCCS2 Completed\n",
      "Year 2012, Header PRCCS3 Completed\n",
      "Year 2012, Header PRCCS4 Completed\n",
      "Year 2012, Header PRCCS5 Completed\n",
      "Year 2012, Header PRCCS6 Completed\n",
      "Year 2012, Header PRCCS7 Completed\n",
      "Year 2012, Header PRCCS8 Completed\n",
      "Year 2012, Header PRCCS9 Completed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-81fab34ed8e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mheaders_found\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'{0}_data'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mheaders_found\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Year {0}, Header {1} Completed\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mmspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mfspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;31m# Patch up the output for NumPy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "headers_found = {}\n",
    "\n",
    "for year in data.keys():\n",
    "    for header_idx, header_pre in enumerate(data[year]['{0}_headers'.format(year)]):\n",
    "        header = header_pre.decode('utf-8').upper()\n",
    "            \n",
    "        if find_feature(header):\n",
    "            \n",
    "            if not (header in headers_found.keys()):\n",
    "                headers_found[header] = {}\n",
    "\n",
    "            series = data[year]['{0}_data'.format(year)][:, header_idx]\n",
    "            headers_found[header][year] = stats(series)\n",
    "            print(\"Year {0}, Header {1} Completed\".format(year, header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field: AMONTH :: Year: 2012 :: Number Missing: 9539\n",
      "Field: AMONTH :: Year: 2013 :: Number Missing: 7535\n",
      "Field: AMONTH :: Year: 2014 :: Number Missing: 7065\n",
      "Field: AWEEKEND :: Year: 2012 :: Number Missing: 1946\n",
      "Field: AWEEKEND :: Year: 2013 :: Number Missing: 188\n",
      "Field: AWEEKEND :: Year: 2014 :: Number Missing: 13\n",
      "Field: ELECTIVE :: Year: 2012 :: Number Missing: 25000\n",
      "Field: ELECTIVE :: Year: 2013 :: Number Missing: 23980\n",
      "Field: ELECTIVE :: Year: 2014 :: Number Missing: 23410\n",
      "Field: FEMALE :: Year: 2012 :: Number Missing: 968\n",
      "Field: FEMALE :: Year: 2013 :: Number Missing: 1445\n",
      "Field: FEMALE :: Year: 2014 :: Number Missing: 1511\n",
      "Field: TRAN_IN :: Year: 2012 :: Number Missing: 39432\n",
      "Field: TRAN_IN :: Year: 2013 :: Number Missing: 31321\n",
      "Field: TRAN_IN :: Year: 2014 :: Number Missing: 43478\n",
      "Field: TRAN_OUT :: Year: 2012 :: Number Missing: 1814\n",
      "Field: TRAN_OUT :: Year: 2013 :: Number Missing: 2532\n",
      "Field: TRAN_OUT :: Year: 2014 :: Number Missing: 2964\n",
      "Field: ZIPINC_QRTL :: Year: 2012 :: Number Missing: 162514\n",
      "Field: ZIPINC_QRTL :: Year: 2013 :: Number Missing: 158891\n",
      "Field: ZIPINC_QRTL :: Year: 2014 :: Number Missing: 153861\n",
      "Field: PL_NCHS :: Year: 2013 :: Number Missing: 34474\n",
      "Field: PL_NCHS :: Year: 2014 :: Number Missing: 33490\n",
      "\n",
      "Total Missing ::: 767371\n"
     ]
    }
   ],
   "source": [
    "# Print missing values\n",
    "import re\n",
    "missing_num = 0\n",
    "# missing_counts = pd.DataFrame()\n",
    "for header, header_info in headers_found.items():\n",
    "    if not re.search('\\d', header):\n",
    "        for year, year_info in header_info.items():\n",
    "            missing_num += year_info['missing']\n",
    "            if year_info['missing'] > 0:\n",
    "                print(\"Field: {0} :: Year: {1} :: Number Missing: {2}\".format(header, year, year_info['missing']))\n",
    "                \n",
    "print('\\nTotal Missing ::: {0}'.format(missing_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also really curious about these external causes of injury codes / cases..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_MCCS1 :: 2012 :: 6150685\n",
      "E_MCCS1 :: 2013 :: 5950562\n",
      "E_MCCS1 :: 2014 :: 5917440\n",
      "ECODE1 :: 2012 :: 6150685\n",
      "ECODE1 :: 2013 :: 5950562\n",
      "ECODE1 :: 2014 :: 5917440\n",
      "ECODE2 :: 2012 :: 6725049\n",
      "ECODE2 :: 2013 :: 6515150\n",
      "ECODE2 :: 2014 :: 6498329\n",
      "ECODE3 :: 2012 :: 7144155\n",
      "ECODE3 :: 2013 :: 6924624\n",
      "ECODE3 :: 2014 :: 6919133\n",
      "ECODE4 :: 2012 :: 7241727\n",
      "ECODE4 :: 2013 :: 7020036\n",
      "ECODE4 :: 2014 :: 7022280\n"
     ]
    }
   ],
   "source": [
    "for header, header_info in headers_found.items():\n",
    "    if header[0:5] == 'ECODE' or header[0:2] == 'E_':\n",
    "        for year, year_info in header_info.items():\n",
    "            print(\"{0} :: {1} :: {2}\".format(header, year, year_info['missing']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHRON1 :: 2012 :: 6317\n",
      "CHRON2 :: 2012 :: 289597\n",
      "CHRON3 :: 2012 :: 905728\n",
      "CHRON4 :: 2012 :: 1487181\n",
      "CHRON5 :: 2012 :: 2030871\n",
      "CHRON6 :: 2012 :: 2542930\n",
      "CHRON7 :: 2012 :: 3024421\n",
      "CHRON8 :: 2012 :: 3482539\n",
      "CHRON9 :: 2012 :: 3926275\n",
      "CHRON10 :: 2012 :: 4428846\n",
      "CHRON11 :: 2012 :: 4786052\n",
      "CHRON12 :: 2012 :: 5106297\n",
      "CHRON13 :: 2012 :: 5397762\n",
      "CHRON14 :: 2012 :: 5662559\n",
      "CHRON15 :: 2012 :: 5905138\n",
      "CHRON16 :: 2012 :: 6191875\n",
      "CHRON17 :: 2012 :: 6429095\n",
      "CHRON18 :: 2012 :: 6566660\n",
      "CHRON19 :: 2012 :: 6798807\n",
      "CHRON20 :: 2012 :: 6879838\n",
      "CHRON21 :: 2012 :: 6952067\n",
      "CHRON22 :: 2012 :: 7009797\n",
      "CHRON23 :: 2012 :: 7058434\n",
      "CHRON24 :: 2012 :: 7101335\n",
      "CHRON25 :: 2012 :: 7145083\n",
      "CHRONB1 :: 2012 :: 6317\n",
      "CHRONB2 :: 2012 :: 289597\n",
      "CHRONB3 :: 2012 :: 905728\n",
      "CHRONB4 :: 2012 :: 1487181\n",
      "CHRONB5 :: 2012 :: 2030871\n",
      "CHRONB6 :: 2012 :: 2542930\n",
      "CHRONB7 :: 2012 :: 3024421\n",
      "CHRONB8 :: 2012 :: 3482539\n",
      "CHRONB9 :: 2012 :: 3926275\n",
      "CHRONB10 :: 2012 :: 4428846\n",
      "CHRONB11 :: 2012 :: 4786052\n",
      "CHRONB12 :: 2012 :: 5106297\n",
      "CHRONB13 :: 2012 :: 5397762\n",
      "CHRONB14 :: 2012 :: 5662559\n",
      "CHRONB15 :: 2012 :: 5905138\n",
      "CHRONB16 :: 2012 :: 6191875\n",
      "CHRONB17 :: 2012 :: 6429095\n",
      "CHRONB18 :: 2012 :: 6566660\n",
      "CHRONB19 :: 2012 :: 6798807\n",
      "CHRONB20 :: 2012 :: 6879838\n",
      "CHRONB21 :: 2012 :: 6952067\n",
      "CHRONB22 :: 2012 :: 7009797\n",
      "CHRONB23 :: 2012 :: 7058434\n",
      "CHRONB24 :: 2012 :: 7101335\n",
      "CHRONB25 :: 2012 :: 7145083\n"
     ]
    }
   ],
   "source": [
    "for header, header_info in headers_found.items():\n",
    "    if header[0:5] == 'CHRON':\n",
    "        for year, year_info in header_info.items():\n",
    "            if year == '2012':\n",
    "                print(\"{0} :: {1} :: {2}\".format(header, year, year_info['missing']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGE :: 2012 :: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 128] :: [856053, 30925, 28566, 16205, 13915, 12716, 11613, 15990, 10656, 9494, 9769, 10431, 17138, 13220, 16068, 19211, 30586, 28567, 37607, 56747, 55118, 59400, 91503, 62944, 64016, 66178, 69121, 102583, 72491, 74945, 76180, 89025, 102568, 68137, 64166, 61242, 56093, 75804, 51109, 51371, 52276, 54739, 74685, 54003, 55227, 57227, 61198, 88550, 71132, 74563, 78173, 81669, 111322, 84936, 103694, 90200, 90047, 122676, 90955, 92286, 93081, 92727, 125341, 94517, 97991, 111183, 91082, 126932, 96710, 105215, 95310, 91671, 122244, 90617, 90480, 90048, 89132, 119383, 85990, 86382, 85882, 86541, 113984, 83430, 82608, 78538, 73402, 89994, 62512, 55562, 236109, 3341]\n",
      "AGE :: 2013 :: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 128] :: [840499, 29134, 25949, 14834, 12849, 12476, 11073, 15041, 9992, 9068, 9200, 9630, 16383, 12836, 15461, 18652, 28410, 25960, 33761, 51980, 50669, 55253, 87645, 62156, 63264, 64664, 66114, 99508, 71034, 72399, 73406, 87723, 100984, 68681, 64309, 60380, 56120, 73242, 49917, 48243, 48209, 49208, 71160, 52463, 51697, 53184, 56051, 80755, 65271, 70297, 74341, 77173, 106895, 81848, 100634, 86715, 88530, 120981, 90962, 90589, 91958, 92610, 124653, 91505, 93269, 103981, 108980, 125805, 89823, 97638, 99982, 93433, 122131, 87898, 88036, 89482, 85674, 115993, 85404, 81554, 80759, 80699, 107791, 80432, 76714, 74077, 70387, 84632, 60287, 54436, 232094, 1584]\n",
      "AGE :: 2014 :: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 128] :: [849551, 26723, 23931, 13937, 12618, 11410, 10930, 15426, 10123, 8994, 9095, 9409, 16320, 13126, 15664, 18641, 28046, 25079, 32100, 49229, 48299, 52377, 85764, 61600, 64655, 66560, 67626, 101354, 72841, 74200, 74025, 89490, 103970, 70632, 67350, 62589, 57728, 75596, 49761, 48266, 46986, 47045, 66759, 53041, 53136, 52896, 55008, 76470, 62095, 67351, 72704, 76358, 105283, 82499, 102134, 87485, 89643, 122734, 92295, 93771, 94573, 94318, 127881, 93771, 92590, 100721, 103966, 145892, 90027, 93639, 94223, 100150, 125479, 88743, 87253, 88756, 86306, 113597, 83232, 82884, 78098, 78111, 103788, 77449, 75696, 70785, 68839, 84063, 58452, 52852, 232552, 2348]\n"
     ]
    }
   ],
   "source": [
    "for header, header_info in headers_found.items():\n",
    "    if header[0:3] == 'AGE':\n",
    "        for year, year_info in header_info.items():\n",
    "            print(\"{0} :: {1} :: {3} :: {4}\".format(header, year, year_info['missing'], year_info['unique'], year_info['counts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, about a million cases that are related to 'external causes of injury'. May as well keep them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides these features, we should create some exclusion criteria:\n",
    "1. Patients under the age of 18.\n",
    "2. Patients with missing values for the following fields: Elective admission, Gender, Transfer In status, Zipcode income quartile,  Admission month, Admission weekend, and PLCHS (urban/rural classification scheme)\n",
    "\n",
    "Other modifications to consider:\n",
    "1. Combine PRDAYn with one-hot encoding for PRn.\n",
    "2. One-hot encode all DX codes. Eek.\n",
    "3. One-hot encode all PR codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHRONB1\n",
      "CHRONB2\n",
      "CHRONB3\n",
      "CHRONB4\n",
      "CHRONB5\n",
      "CHRONB6\n",
      "CHRONB7\n",
      "CHRONB8\n",
      "CHRONB9\n",
      "CHRONB10\n",
      "CHRONB11\n",
      "CHRONB12\n",
      "CHRONB13\n",
      "CHRONB14\n",
      "CHRONB15\n",
      "CHRONB16\n",
      "CHRONB17\n",
      "CHRONB18\n",
      "CHRONB19\n",
      "CHRONB20\n",
      "CHRONB21\n",
      "CHRONB22\n",
      "CHRONB23\n",
      "CHRONB24\n",
      "CHRONB25\n",
      "PL_NCHS2006\n",
      "PL_NCHS\n",
      "DX26\n",
      "DX27\n",
      "DX28\n",
      "DX29\n",
      "DX30\n",
      "DXCCS26\n",
      "DXCCS27\n",
      "DXCCS28\n",
      "DXCCS29\n",
      "DXCCS30\n",
      "BODYSYSTEM1\n",
      "BODYSYSTEM2\n",
      "BODYSYSTEM3\n",
      "BODYSYSTEM4\n",
      "BODYSYSTEM5\n",
      "BODYSYSTEM6\n",
      "BODYSYSTEM7\n",
      "BODYSYSTEM8\n",
      "BODYSYSTEM9\n",
      "BODYSYSTEM10\n",
      "BODYSYSTEM11\n",
      "BODYSYSTEM12\n",
      "BODYSYSTEM13\n",
      "BODYSYSTEM14\n",
      "BODYSYSTEM15\n",
      "BODYSYSTEM16\n",
      "BODYSYSTEM17\n",
      "BODYSYSTEM18\n",
      "BODYSYSTEM19\n",
      "BODYSYSTEM20\n",
      "BODYSYSTEM21\n",
      "BODYSYSTEM22\n",
      "BODYSYSTEM23\n",
      "BODYSYSTEM24\n",
      "BODYSYSTEM25\n",
      "BODYSYSTEM26\n",
      "BODYSYSTEM27\n",
      "BODYSYSTEM28\n",
      "BODYSYSTEM29\n",
      "BODYSYSTEM30\n",
      "CHRON26\n",
      "CHRON27\n",
      "CHRON28\n",
      "CHRON29\n",
      "CHRON30\n"
     ]
    }
   ],
   "source": [
    "for header, header_info in headers_found.items():\n",
    "    if len(header_info.keys()) < 3:\n",
    "        print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably better to generate one-hot encodings of minibatches at train time rather than store it all in one big go now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: data_proc.close()\n",
    "except: pass\n",
    "\n",
    "DATA_PRUNED_FN = 'NIS_Pruned.h5'\n",
    "data_proc = tables.open_file(DATA_FOLDER + DATA_PRUNED_FN, 'w')\n",
    "\n",
    "data_proc.create_earray('/', 'key_nis', shape=(0, ), atom=tables.Int32Atom());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(DATA_FOLDER + 'NIS_columnstats_2012_2014.json', 'r') as f:\n",
    "    headers_found = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 0\n",
    "for year, year_info in data.items():\n",
    "    year_data = year_info['{0}_data'.format(year)]\n",
    "    num_records += year_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21438293"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year :: 2012, Key :: b'AGE', Excluded Num :: 1188730\n",
      "Year :: 2012, Key :: b'ELECTIVE', Excluded Num :: 25000\n",
      "Year :: 2012, Key :: b'FEMALE', Excluded Num :: 968\n",
      "Year :: 2012, Key :: b'TRAN_IN', Excluded Num :: 39432\n",
      "Year :: 2012, Key :: b'ZIPINC_QRTL', Excluded Num :: 162514\n",
      "Year :: 2012, Key :: b'HCUP_ED', Excluded Num :: 0\n",
      "Year :: 2013, Key :: b'AGE', Excluded Num :: 1151208\n",
      "Year :: 2013, Key :: b'ELECTIVE', Excluded Num :: 23980\n",
      "Year :: 2013, Key :: b'FEMALE', Excluded Num :: 1445\n",
      "Year :: 2013, Key :: b'TRAN_IN', Excluded Num :: 31321\n",
      "Year :: 2013, Key :: b'ZIPINC_QRTL', Excluded Num :: 158891\n",
      "Year :: 2013, Key :: b'HCUP_ED', Excluded Num :: 0\n",
      "Year :: 2014, Key :: b'AGE', Excluded Num :: 1151123\n",
      "Year :: 2014, Key :: b'ELECTIVE', Excluded Num :: 23410\n",
      "Year :: 2014, Key :: b'FEMALE', Excluded Num :: 1511\n",
      "Year :: 2014, Key :: b'TRAN_IN', Excluded Num :: 43478\n",
      "Year :: 2014, Key :: b'ZIPINC_QRTL', Excluded Num :: 153861\n",
      "Year :: 2014, Key :: b'HCUP_ED', Excluded Num :: 0\n"
     ]
    }
   ],
   "source": [
    "# keys_to_remove = np.empty((0, ), dtype='int32')\n",
    "keys_to_remove = {}\n",
    "remove_missing = (lambda series : np.where(series == -128))\n",
    "\n",
    "# Define exclusion criteria\n",
    "exclusion_criteria = {\n",
    "    b'AGE' : (lambda series : np.where(series < 19)),\n",
    "    b'ELECTIVE' : remove_missing,\n",
    "    b'FEMALE' : remove_missing,\n",
    "    b'TRAN_IN' : remove_missing,\n",
    "    b'ZIPINC_QRTL' : remove_missing,\n",
    "    b'HCUP_ED' : remove_missing,\n",
    "#     b'AMONTH' : remove_missing,\n",
    "#     b'AWEEKEND' : remove_missing,\n",
    "#     b'PL_NCHS' : remove_missing,\n",
    "#     b'PL_NCHS2006' : remove_missing,\n",
    "}\n",
    "\n",
    "excluded = 0\n",
    "\n",
    "# Execute exclusion criteria\n",
    "for year, year_info in data.items():\n",
    "    year_headers = year_info['{0}_headers'.format(year)][:].tolist()\n",
    "    year_data = year_info['{0}_data'.format(year)]\n",
    "    \n",
    "    keys_to_remove_yr = np.empty((0, ), dtype='int32')\n",
    "    nis_keys = year_data[:, year_headers.index(b'KEY_NIS')]\n",
    "    \n",
    "    for exclusion_key, exclusion_func in exclusion_criteria.items():\n",
    "        try:\n",
    "            exclusion_idx = year_headers.index(exclusion_key)\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "        exclusion_series = year_data[:, exclusion_idx]\n",
    "        excluded_idxs = exclusion_func(exclusion_series)\n",
    "        excluded_keys = nis_keys[excluded_idxs]\n",
    "        \n",
    "        keys_to_remove_yr = np.concatenate((keys_to_remove_yr, excluded_keys))\n",
    "        \n",
    "        print('Year :: {2}, Key :: {0}, Excluded Num :: {1}'.format(exclusion_key, len(excluded_keys), year))\n",
    "        excluded += len(excluded_keys)\n",
    "        \n",
    "    keys_to_remove[year] = keys_to_remove_yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17281421"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_records - excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc_headers = []\n",
    "outcome_headers = []\n",
    "\n",
    "for year, year_info in data.items():\n",
    "    year_headers = year_info['{0}_headers'.format(year)][:].tolist()\n",
    "#     year_header_idx_map = (-1 * np.ones(len(data_proc_headers), dtype='int16')).tolist() # Array that will store where year_headers lie in the data_proc_header list.\n",
    "\n",
    "\n",
    "    for year_header_idx, year_header in enumerate(year_headers):\n",
    "\n",
    "        # If we care about this header as an input feature...\n",
    "        if year_header.decode('utf-8').upper() in headers_found.keys():\n",
    "            \n",
    "            if year_header == b'PL_NCHS2006':\n",
    "                year_header_rectified = b'PL_NCHS'\n",
    "                \n",
    "            elif year_header[0:10] == b'BODYSYSTEM':\n",
    "                n_idx = re.search('\\d', year_header.decode('utf-8').upper()).start()\n",
    "                n = int(year_header[n_idx:].decode('utf-8'))\n",
    "                year_header_rectified = ('CHRONB{0:02d}'.format(n)).encode('ASCII')\n",
    "            \n",
    "            elif len(re.findall('\\d', year_header.decode('utf-8'))) > 0:\n",
    "                n_idx = re.search('\\d', year_header.decode('utf-8').upper()).start()\n",
    "                prefix = re.split('\\d', year_header.decode('utf-8').upper())[0]\n",
    "                n = int(year_header[n_idx:].decode('utf-8'))\n",
    "                year_header_rectified = ('{0}{1:02d}'.format(prefix, n)).encode('ASCII')                \n",
    "            \n",
    "            else:\n",
    "                year_header_rectified = year_header\n",
    "\n",
    "            # If this key we need to include isn't already present...\n",
    "            if not (year_header_rectified in data_proc_headers):\n",
    "                data_proc_headers.append(year_header_rectified)\n",
    "                \n",
    "        elif year_header.decode('utf-8').upper() in OUTCOMES:\n",
    "            if year_header not in outcome_headers:\n",
    "                outcome_headers.append(year_header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/outcomes (EArray(0, 8)) ''\n",
       "  atom := Int32Atom(shape=(), dflt=0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (2048, 8)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, let's make sure all column names are consistent.\n",
    "\n",
    "# data_proc_headers = []\n",
    "year_idx_maps = []\n",
    "excluded_idx_maps = []\n",
    "\n",
    "for year, year_info in data.items():\n",
    "    year_data = year_info['{0}_data'.format(year)]    \n",
    "    year_keys = year_data[:, year_headers.index(b'KEY_NIS')]\n",
    "    \n",
    "    # Now, let's ensure that our column indices line up appropriately with the current state of the header list.\n",
    "    year_headers = year_info['{0}_headers'.format(year)][:].tolist()\n",
    "    year_header_idx_map = (-1 * np.ones(len(data_proc_headers), dtype='int16')).tolist() # Array that will store where year_headers lie in the data_proc_header list.\n",
    "    excluded_idx_map = (-1 * np.ones(len(outcome_headers), dtype='int16')).tolist()\n",
    "    \n",
    "    for year_header_idx, year_header in enumerate(year_headers):\n",
    "\n",
    "        # If we care about this header as an input feature...\n",
    "        if year_header.decode('utf-8').upper() in headers_found.keys():\n",
    "            \n",
    "            if year_header == b'PL_NCHS2006':\n",
    "                year_header_rectified = b'PL_NCHS'\n",
    "                \n",
    "            elif year_header[0:10] == b'BODYSYSTEM':\n",
    "                n_idx = re.search('\\d', year_header.decode('utf-8').upper()).start()\n",
    "                n = int(year_header[n_idx:].decode('utf-8'))\n",
    "                year_header_rectified = ('CHRONB{0:02d}'.format(n)).encode('ASCII')\n",
    "            \n",
    "            elif len(re.findall('\\d', year_header.decode('utf-8'))) > 0:\n",
    "                n_idx = re.search('\\d', year_header.decode('utf-8').upper()).start()\n",
    "                prefix = re.split('\\d', year_header.decode('utf-8').upper())[0]\n",
    "                n = int(year_header[n_idx:].decode('utf-8'))\n",
    "                year_header_rectified = ('{0}{1:02d}'.format(prefix, n)).encode('ASCII')                \n",
    "            \n",
    "            else:\n",
    "                year_header_rectified = year_header\n",
    "\n",
    "            # If this key we need to include isn't already present...\n",
    "            if not (year_header_rectified in data_proc_headers):\n",
    "                year_header_idx_map.append(year_header_idx)\n",
    "                    \n",
    "            else:\n",
    "                year_header_idx_map[data_proc_headers.index(year_header_rectified)] = year_header_idx\n",
    "                \n",
    "        elif year_header.decode('utf-8').upper() in OUTCOMES:\n",
    "            if year_header not in outcome_headers:\n",
    "                excluded_idx_map.append(year_header_idx)\n",
    "            else:\n",
    "                excluded_idx_map[outcome_headers.index(year_header)] = year_header_idx\n",
    "                \n",
    "                \n",
    "    year_idx_maps.append(year_header_idx_map)\n",
    "    excluded_idx_maps.append(excluded_idx_map)\n",
    "\n",
    "for i, year_idx_map in enumerate(year_idx_maps):\n",
    "    diff_in_len = len(data_proc_headers) - len(year_idx_map)\n",
    "    if diff_in_len > 0:\n",
    "        extension_len = [-1] * diff_in_len\n",
    "        year_idx_maps[i].extend(extension_len)\n",
    "        \n",
    "sort_idx = np.argsort(data_proc_headers)\n",
    "data_proc_headers = np.sort(data_proc_headers)\n",
    "\n",
    "year_idx_maps_sorted = []\n",
    "for i, year_idx_map in enumerate(year_idx_maps):\n",
    "    year_idx_maps_sorted.append(np.array(year_idx_map)[sort_idx].tolist())\n",
    "    \n",
    "sort_idx = np.argsort(outcome_headers)\n",
    "outcome_headers = np.sort(outcome_headers)\n",
    "\n",
    "excluded_idx_maps_sorted = []\n",
    "for i, excluded_idx_map in enumerate(excluded_idx_maps):\n",
    "    excluded_idx_maps_sorted.append(np.array(excluded_idx_map)[sort_idx].tolist())\n",
    "\n",
    "data_proc.create_array('/', 'headers', data_proc_headers)\n",
    "data_proc.create_array('/', 'outcome_headers', outcome_headers)\n",
    "data_proc.create_earray('/', 'dataset', shape=(0, len(data_proc_headers)), atom=tables.Int32Atom())\n",
    "data_proc.create_earray('/', 'outcomes', shape=(0, len(outcome_headers)), atom=tables.Int32Atom())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0 : 1000000\n",
      "Starting 1000000 : 2000000\n",
      "Starting 2000000 : 3000000\n",
      "Starting 3000000 : 4000000\n",
      "Starting 4000000 : 5000000\n",
      "Starting 5000000 : 6000000\n",
      "Starting 6000000 : 7000000\n",
      "Starting 7000000 : 8000000\n",
      "Starting 0 : 1000000\n",
      "Starting 1000000 : 2000000\n",
      "Starting 2000000 : 3000000\n",
      "Starting 3000000 : 4000000\n",
      "Starting 4000000 : 5000000\n",
      "Starting 5000000 : 6000000\n",
      "Starting 6000000 : 7000000\n",
      "Starting 7000000 : 8000000\n",
      "Starting 0 : 1000000\n",
      "Starting 1000000 : 2000000\n",
      "Starting 2000000 : 3000000\n",
      "Starting 3000000 : 4000000\n",
      "Starting 4000000 : 5000000\n",
      "Starting 5000000 : 6000000\n",
      "Starting 6000000 : 7000000\n",
      "Starting 7000000 : 8000000\n"
     ]
    }
   ],
   "source": [
    "# Now, let's remove excluded cases and combine all years in a single dataset! \n",
    "\n",
    "for idx, (year, year_info) in enumerate(data.items()):\n",
    "\n",
    "    year_headers = year_info['{0}_headers'.format(year)][:].tolist()\n",
    "    year_data = year_info['{0}_data'.format(year)]\n",
    "\n",
    "    year_idx_map = year_idx_maps_sorted[idx]\n",
    "    excluded_idx_map = excluded_idx_maps_sorted[idx]\n",
    "    keys_to_remove_yr = keys_to_remove[year]\n",
    "    \n",
    "    count = 0\n",
    "    chunk_size = 1000000\n",
    "    while count < year_data.shape[0]:\n",
    "        print(\"Starting {0} : {1}\".format(count, count + chunk_size))\n",
    "\n",
    "        if count > year_data.shape[0]:\n",
    "            count = year_data.shape[0]\n",
    "\n",
    "        chunk = year_data[count:count+chunk_size, :]\n",
    "        \n",
    "        # Do something\n",
    "        year_keys = chunk[:, year_headers.index(b'KEY_NIS')]\n",
    "\n",
    "        # Remove keys\n",
    "        key_int_to_remove = np.in1d(year_keys, keys_to_remove_yr)\n",
    "        #     idxs_to_remove = np.where(key_int_to_remove)\n",
    "\n",
    "        key_int_to_keep = ~key_int_to_remove\n",
    "        idxs_to_keep = np.where(key_int_to_keep)[0]\n",
    "        \n",
    "        year_data_pruned_nocols = np.take(chunk, idxs_to_keep, axis=0)\n",
    "        year_data_pruned_nocols_ext = np.concatenate([year_data_pruned_nocols, (0 * np.ones((year_data_pruned_nocols.shape[0], 1))).astype('int32')], axis=1)\n",
    "        year_data_pruned_cols = np.take(year_data_pruned_nocols_ext, year_idx_map, axis=1)\n",
    "        \n",
    "        outcome_year = np.take(year_data_pruned_nocols_ext, excluded_idx_map, axis=1)\n",
    "        \n",
    "        data_proc.root.dataset.append(year_data_pruned_cols)\n",
    "        data_proc.root.key_nis.append(year_keys[idxs_to_keep])\n",
    "        \n",
    "        data_proc.root.outcomes.append(outcome_year)\n",
    "        \n",
    "          \n",
    "        # Done\n",
    "        count += chunk_size\n",
    "        if count == year_data.shape[0]:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc = tables.open_file(DATA_FOLDER + DATA_PRUNED_FN, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    1,    0,    0,    1,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    1,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    1,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    1,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    1,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    1,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0, -128,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    1,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    1,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0, -128,    0,    0,    0,\n",
       "          0,    0,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    1,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    1,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    1,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    1,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_proc.root.outcomes[0:1000, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KEY_NIS is NOT a unique number across other years!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Out Admission Diagnoses vs. Pre-existing Comorbidities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import re\n",
    "import tables\n",
    "import json\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data/raw/'\n",
    "MISS_VAL_FILL = -128\n",
    "YEARS = ['2012', '2013', '2014']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc = tables.open_file(DATA_FOLDER + 'NIS_Pruned.h5', 'r')\n",
    "dataset = data_proc.root.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: data_proc.close()\n",
    "except: pass\n",
    "\n",
    "# data_proc = tables.open_file(DATA_FOLDER + 'NIS_2012_2014_proto_emb_red.h5', 'w')\n",
    "# dataset = data_proc.root.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chron_idxs = []\n",
    "dx_idxs = []\n",
    "for header_idx, header in enumerate(data_proc.root.headers):\n",
    "    if header[:5] == b'CHRON' and header[5:6] != b'B':\n",
    "        chron_idxs.append(header_idx)\n",
    "    if header[:2] == b'DX' and header[2:3] != b'C':\n",
    "        dx_idxs.append(header_idx)\n",
    "        \n",
    "chron_idxs = np.array(chron_idxs, dtype='uint16')\n",
    "dx_idxs = np.array(dx_idxs, dtype='uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit(nopython=True)\n",
    "def remove_chrons_from_dxs(sample, chron_idxs, dx_idxs):\n",
    "    sample_chrons = sample[:, chron_idxs].ravel()\n",
    "    sample_dxs = sample[:, dx_idxs].ravel()\n",
    "    mask = (sample_chrons == 1)\n",
    "    sample_chrons = sample_dxs * mask\n",
    "    mask.reshape(sample.shape[0], dx_idxs.shape[0])[:, 0] = 0\n",
    "    mask = mask.ravel()\n",
    "    sample_dxs[mask] = 0\n",
    "    sample[:, dx_idxs] = sample_dxs.reshape(sample.shape[0], dx_idxs.shape[0])\n",
    "    sample[:, chron_idxs] = sample_chrons.reshape(sample.shape[0], chron_idxs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 ms Â± 1.35 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit remove_chrons_from_dxs(sample, chron_idxs, dx_idxs) # JIT result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169 Âµs Â± 302 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit remove_chrons_from_dxs(sample, chron_idxs, dx_idxs).reshape(sample_chrons.shape) # regular result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0 : 1000000\n",
      "Starting 1000000 : 2000000\n",
      "Starting 2000000 : 3000000\n",
      "Starting 3000000 : 4000000\n",
      "Starting 4000000 : 5000000\n",
      "Starting 5000000 : 6000000\n",
      "Starting 6000000 : 7000000\n",
      "Starting 7000000 : 8000000\n",
      "Starting 8000000 : 9000000\n",
      "Starting 9000000 : 10000000\n",
      "Starting 10000000 : 11000000\n",
      "Starting 11000000 : 12000000\n",
      "Starting 12000000 : 13000000\n",
      "Starting 13000000 : 14000000\n",
      "Starting 14000000 : 15000000\n",
      "Starting 15000000 : 16000000\n",
      "Starting 16000000 : 17000000\n",
      "Starting 17000000 : 18000000\n"
     ]
    }
   ],
   "source": [
    "dataset = data_proc.root.dataset\n",
    "\n",
    "count = 0\n",
    "chunk_size = 1000000\n",
    "\n",
    "while count < dataset.shape[0]:\n",
    "\n",
    "    print(\"Starting {0} : {1}\".format(count, count + chunk_size))\n",
    "\n",
    "    if count > dataset.shape[0]:\n",
    "        count = dataset.shape[0]\n",
    "\n",
    "    sample = dataset[count:count+chunk_size, :]\n",
    "    remove_chrons_from_dxs(sample, chron_idxs, dx_idxs)\n",
    "    dataset[count:count+chunk_size, :] = sample\n",
    "\n",
    "    count += chunk_size\n",
    "    if count == dataset.shape[0]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Absent Procedures / Diagnoses to 0 for Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,\n",
       "        16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
       "        29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,\n",
       "        55,  56,  57,  58,  59,  60,  61,  62,  92,  93,  94,  95,  96,\n",
       "        97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
       "       110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 152,\n",
       "       153, 154, 155], dtype=uint16)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_idxs = []\n",
    "for header_idx, header in enumerate(data_proc.root.headers):\n",
    "    if header[:6] == b'PR' or (header[:2] == b'DX' and header[2:3] != b'C') or (header[:5] == b'ECODE') or (header[:5] == b'CHRON'):\n",
    "        embedding_idxs.append(header_idx)\n",
    "        \n",
    "embedding_idxs = np.array(embedding_idxs, dtype='uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def remove_negative_vals(sample, embedding_idxs):\n",
    "    embeds = sample[:, embedding_idxs].ravel()\n",
    "    mask = (embeds == -128)\n",
    "    embeds[mask] = 0\n",
    "    sample[:, embedding_idxs] = embeds.reshape(sample.shape[0], embedding_idxs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0 : 1000000\n",
      "Starting 1000000 : 2000000\n",
      "Starting 2000000 : 3000000\n",
      "Starting 3000000 : 4000000\n",
      "Starting 4000000 : 5000000\n",
      "Starting 5000000 : 6000000\n",
      "Starting 6000000 : 7000000\n",
      "Starting 7000000 : 8000000\n",
      "Starting 8000000 : 9000000\n",
      "Starting 9000000 : 10000000\n",
      "Starting 10000000 : 11000000\n",
      "Starting 11000000 : 12000000\n",
      "Starting 12000000 : 13000000\n",
      "Starting 13000000 : 14000000\n",
      "Starting 14000000 : 15000000\n",
      "Starting 15000000 : 16000000\n",
      "Starting 16000000 : 17000000\n",
      "Starting 17000000 : 18000000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "chunk_size = 1000000\n",
    "while count < dataset.shape[0]:\n",
    "\n",
    "    print(\"Starting {0} : {1}\".format(count, count + chunk_size))\n",
    "    \n",
    "    # iteration\n",
    "    if count > dataset.shape[0]:\n",
    "        count = dataset.shape[0]\n",
    "    \n",
    "    # do something\n",
    "    chunk = dataset[count:count+chunk_size, :]\n",
    "    remove_negative_vals(chunk, embedding_idxs)\n",
    "    dataset[count:count+chunk_size, :] = chunk\n",
    "\n",
    "    # finalize iteration\n",
    "    count += chunk_size\n",
    "    if count == dataset.shape[0]:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Extra Columns for Our First Autoencoder Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers_to_keep = [b'DXn', b'PRn', b'ECODEn', b'ELECTIVE', b'FEMALE', b'AGE', b'HCUP_ED', b'ZIPINC_QRTL', b'CHRONBn', b'CHRONn', b'TRAN_IN', 'CHRON']\n",
    "headers_to_keep = [b'DXn', b'DXCCSn', b'PRn', b'ECODEn', b'ELECTIVE', b'FEMALE', b'AGE', b'HCUP_ED', b'ZIPINC_QRTL', b'CHRONBn', b'CHRONn', b'TRAN_IN', 'CHRON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_feature(header, keepers):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "        \n",
    "    if header in keepers:\n",
    "        return True\n",
    "                \n",
    "    else:\n",
    "        header = header.decode('utf-8')\n",
    "        prefix = re.split('\\d', header)[0] # Prefix before #\n",
    "        n_feature = (prefix + 'n').encode('ASCII')\n",
    "        \n",
    "        if n_feature in keepers:\n",
    "            return n_feature\n",
    "        \n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc = tables.open_file(DATA_FOLDER + 'NIS_Pruned.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_keep_all = []\n",
    "for header in data_proc.root.headers:\n",
    "    if find_feature(header, headers_to_keep):\n",
    "        headers_to_keep_all.append(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'AGE',\n",
       " b'CHRON01',\n",
       " b'CHRON02',\n",
       " b'CHRON03',\n",
       " b'CHRON04',\n",
       " b'CHRON05',\n",
       " b'CHRON06',\n",
       " b'CHRON07',\n",
       " b'CHRON08',\n",
       " b'CHRON09',\n",
       " b'CHRON10',\n",
       " b'CHRON11',\n",
       " b'CHRON12',\n",
       " b'CHRON13',\n",
       " b'CHRON14',\n",
       " b'CHRON15',\n",
       " b'CHRON16',\n",
       " b'CHRON17',\n",
       " b'CHRON18',\n",
       " b'CHRON19',\n",
       " b'CHRON20',\n",
       " b'CHRON21',\n",
       " b'CHRON22',\n",
       " b'CHRON23',\n",
       " b'CHRON24',\n",
       " b'CHRON25',\n",
       " b'CHRON26',\n",
       " b'CHRON27',\n",
       " b'CHRON28',\n",
       " b'CHRON29',\n",
       " b'CHRON30',\n",
       " b'CHRONB01',\n",
       " b'CHRONB02',\n",
       " b'CHRONB03',\n",
       " b'CHRONB04',\n",
       " b'CHRONB05',\n",
       " b'CHRONB06',\n",
       " b'CHRONB07',\n",
       " b'CHRONB08',\n",
       " b'CHRONB09',\n",
       " b'CHRONB10',\n",
       " b'CHRONB11',\n",
       " b'CHRONB12',\n",
       " b'CHRONB13',\n",
       " b'CHRONB14',\n",
       " b'CHRONB15',\n",
       " b'CHRONB16',\n",
       " b'CHRONB17',\n",
       " b'CHRONB18',\n",
       " b'CHRONB19',\n",
       " b'CHRONB20',\n",
       " b'CHRONB21',\n",
       " b'CHRONB22',\n",
       " b'CHRONB23',\n",
       " b'CHRONB24',\n",
       " b'CHRONB25',\n",
       " b'CHRONB26',\n",
       " b'CHRONB27',\n",
       " b'CHRONB28',\n",
       " b'CHRONB29',\n",
       " b'CHRONB30',\n",
       " b'DX01',\n",
       " b'DX02',\n",
       " b'DX03',\n",
       " b'DX04',\n",
       " b'DX05',\n",
       " b'DX06',\n",
       " b'DX07',\n",
       " b'DX08',\n",
       " b'DX09',\n",
       " b'DX10',\n",
       " b'DX11',\n",
       " b'DX12',\n",
       " b'DX13',\n",
       " b'DX14',\n",
       " b'DX15',\n",
       " b'DX16',\n",
       " b'DX17',\n",
       " b'DX18',\n",
       " b'DX19',\n",
       " b'DX20',\n",
       " b'DX21',\n",
       " b'DX22',\n",
       " b'DX23',\n",
       " b'DX24',\n",
       " b'DX25',\n",
       " b'DX26',\n",
       " b'DX27',\n",
       " b'DX28',\n",
       " b'DX29',\n",
       " b'DX30',\n",
       " b'DXCCS01',\n",
       " b'DXCCS02',\n",
       " b'DXCCS03',\n",
       " b'DXCCS04',\n",
       " b'DXCCS05',\n",
       " b'DXCCS06',\n",
       " b'DXCCS07',\n",
       " b'DXCCS08',\n",
       " b'DXCCS09',\n",
       " b'DXCCS10',\n",
       " b'DXCCS11',\n",
       " b'DXCCS12',\n",
       " b'DXCCS13',\n",
       " b'DXCCS14',\n",
       " b'DXCCS15',\n",
       " b'DXCCS16',\n",
       " b'DXCCS17',\n",
       " b'DXCCS18',\n",
       " b'DXCCS19',\n",
       " b'DXCCS20',\n",
       " b'DXCCS21',\n",
       " b'DXCCS22',\n",
       " b'DXCCS23',\n",
       " b'DXCCS24',\n",
       " b'DXCCS25',\n",
       " b'DXCCS26',\n",
       " b'DXCCS27',\n",
       " b'DXCCS28',\n",
       " b'DXCCS29',\n",
       " b'DXCCS30',\n",
       " b'ECODE01',\n",
       " b'ECODE02',\n",
       " b'ECODE03',\n",
       " b'ECODE04',\n",
       " b'ELECTIVE',\n",
       " b'FEMALE',\n",
       " b'HCUP_ED',\n",
       " b'PR01',\n",
       " b'PR02',\n",
       " b'PR03',\n",
       " b'PR04',\n",
       " b'PR05',\n",
       " b'PR06',\n",
       " b'PR07',\n",
       " b'PR08',\n",
       " b'PR09',\n",
       " b'PR10',\n",
       " b'PR11',\n",
       " b'PR12',\n",
       " b'PR13',\n",
       " b'PR14',\n",
       " b'PR15',\n",
       " b'TRAN_IN',\n",
       " b'ZIPINC_QRTL']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers_to_keep_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_data = tables.open_file(DATA_FOLDER + 'NIS_2012_2014_prototype_v2_ccs.h5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/dataset (CArray(17369821, 145)) ''\n",
       "  atom := Int32Atom(shape=(), dflt=0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (903, 145)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prototype_data.create_carray('/', 'dataset', atom=tables.Int32Atom(), shape=(data_proc.root.dataset.shape[0], len(headers_to_keep_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 b'AGE' 0\n",
      "1 b'CHRON01' 3\n",
      "2 b'CHRON02' 4\n",
      "3 b'CHRON03' 5\n",
      "4 b'CHRON04' 6\n",
      "5 b'CHRON05' 7\n",
      "6 b'CHRON06' 8\n",
      "7 b'CHRON07' 9\n",
      "8 b'CHRON08' 10\n",
      "9 b'CHRON09' 11\n",
      "10 b'CHRON10' 12\n",
      "11 b'CHRON11' 13\n",
      "12 b'CHRON12' 14\n",
      "13 b'CHRON13' 15\n",
      "14 b'CHRON14' 16\n",
      "15 b'CHRON15' 17\n",
      "16 b'CHRON16' 18\n",
      "17 b'CHRON17' 19\n",
      "18 b'CHRON18' 20\n",
      "19 b'CHRON19' 21\n",
      "20 b'CHRON20' 22\n",
      "21 b'CHRON21' 23\n",
      "22 b'CHRON22' 24\n",
      "23 b'CHRON23' 25\n",
      "24 b'CHRON24' 26\n",
      "25 b'CHRON25' 27\n",
      "26 b'CHRON26' 28\n",
      "27 b'CHRON27' 29\n",
      "28 b'CHRON28' 30\n",
      "29 b'CHRON29' 31\n",
      "30 b'CHRON30' 32\n",
      "31 b'CHRONB01' 33\n",
      "32 b'CHRONB02' 34\n",
      "33 b'CHRONB03' 35\n",
      "34 b'CHRONB04' 36\n",
      "35 b'CHRONB05' 37\n",
      "36 b'CHRONB06' 38\n",
      "37 b'CHRONB07' 39\n",
      "38 b'CHRONB08' 40\n",
      "39 b'CHRONB09' 41\n",
      "40 b'CHRONB10' 42\n",
      "41 b'CHRONB11' 43\n",
      "42 b'CHRONB12' 44\n",
      "43 b'CHRONB13' 45\n",
      "44 b'CHRONB14' 46\n",
      "45 b'CHRONB15' 47\n",
      "46 b'CHRONB16' 48\n",
      "47 b'CHRONB17' 49\n",
      "48 b'CHRONB18' 50\n",
      "49 b'CHRONB19' 51\n",
      "50 b'CHRONB20' 52\n",
      "51 b'CHRONB21' 53\n",
      "52 b'CHRONB22' 54\n",
      "53 b'CHRONB23' 55\n",
      "54 b'CHRONB24' 56\n",
      "55 b'CHRONB25' 57\n",
      "56 b'CHRONB26' 58\n",
      "57 b'CHRONB27' 59\n",
      "58 b'CHRONB28' 60\n",
      "59 b'CHRONB29' 61\n",
      "60 b'CHRONB30' 62\n",
      "61 b'DX01' 92\n",
      "62 b'DX02' 93\n",
      "63 b'DX03' 94\n",
      "64 b'DX04' 95\n",
      "65 b'DX05' 96\n",
      "66 b'DX06' 97\n",
      "67 b'DX07' 98\n",
      "68 b'DX08' 99\n",
      "69 b'DX09' 100\n",
      "70 b'DX10' 101\n",
      "71 b'DX11' 102\n",
      "72 b'DX12' 103\n",
      "73 b'DX13' 104\n",
      "74 b'DX14' 105\n",
      "75 b'DX15' 106\n",
      "76 b'DX16' 107\n",
      "77 b'DX17' 108\n",
      "78 b'DX18' 109\n",
      "79 b'DX19' 110\n",
      "80 b'DX20' 111\n",
      "81 b'DX21' 112\n",
      "82 b'DX22' 113\n",
      "83 b'DX23' 114\n",
      "84 b'DX24' 115\n",
      "85 b'DX25' 116\n",
      "86 b'DX26' 117\n",
      "87 b'DX27' 118\n",
      "88 b'DX28' 119\n",
      "89 b'DX29' 120\n",
      "90 b'DX30' 121\n",
      "91 b'DXCCS01' 122\n",
      "92 b'DXCCS02' 123\n",
      "93 b'DXCCS03' 124\n",
      "94 b'DXCCS04' 125\n",
      "95 b'DXCCS05' 126\n",
      "96 b'DXCCS06' 127\n",
      "97 b'DXCCS07' 128\n",
      "98 b'DXCCS08' 129\n",
      "99 b'DXCCS09' 130\n",
      "100 b'DXCCS10' 131\n",
      "101 b'DXCCS11' 132\n",
      "102 b'DXCCS12' 133\n",
      "104 b'DXCCS14' 135\n",
      "105 b'DXCCS15' 136\n",
      "106 b'DXCCS16' 137\n",
      "107 b'DXCCS17' 138\n",
      "108 b'DXCCS18' 139\n",
      "109 b'DXCCS19' 140\n",
      "110 b'DXCCS20' 141\n",
      "111 b'DXCCS21' 142\n",
      "112 b'DXCCS22' 143\n",
      "113 b'DXCCS23' 144\n",
      "114 b'DXCCS24' 145\n",
      "115 b'DXCCS25' 146\n",
      "116 b'DXCCS26' 147\n",
      "117 b'DXCCS27' 148\n",
      "118 b'DXCCS28' 149\n",
      "119 b'DXCCS29' 150\n",
      "120 b'DXCCS30' 151\n",
      "121 b'ECODE01' 152\n",
      "122 b'ECODE02' 153\n",
      "123 b'ECODE03' 154\n",
      "124 b'ECODE04' 155\n",
      "125 b'ELECTIVE' 156\n",
      "126 b'FEMALE' 158\n",
      "127 b'HCUP_ED' 159\n",
      "128 b'PR01' 180\n",
      "129 b'PR02' 181\n",
      "130 b'PR03' 182\n",
      "131 b'PR04' 183\n",
      "132 b'PR05' 184\n",
      "133 b'PR06' 185\n",
      "134 b'PR07' 186\n",
      "135 b'PR08' 187\n",
      "136 b'PR09' 188\n",
      "137 b'PR10' 189\n",
      "138 b'PR11' 190\n",
      "139 b'PR12' 191\n",
      "140 b'PR13' 192\n",
      "141 b'PR14' 193\n",
      "142 b'PR15' 194\n",
      "143 b'TRAN_IN' 226\n",
      "144 b'ZIPINC_QRTL' 228\n"
     ]
    }
   ],
   "source": [
    "for i, header in enumerate(headers_to_keep_all):\n",
    "    idx = data_proc.root.headers[:].tolist().index(header)\n",
    "    print(i, header, idx)\n",
    "    prototype_data.root.dataset[:, i] = dataset[:, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'TRAIN': {'split': 0.6}, \n",
    "          'VAL': {'split': 0.2},\n",
    "          'TEST': {'split': 0.2}\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_inds = np.arange(0, dataset.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=0)\n",
    "\n",
    "for shuffle_x in range(10):\n",
    "    np.random.shuffle(split_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound_ind = 0\n",
    "for subset, subset_info in splits.items():\n",
    "    upper_bound_ind = int(np.round(lower_bound_ind + subset_info['split'] * dataset.shape[0]))\n",
    "    splits[subset]['inds'] = split_inds[lower_bound_ind:upper_bound_ind].tolist()\n",
    "    lower_bound_ind = upper_bound_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_FOLDER + 'NIS_2012_2014_prototype_TVTsplit_v2_ccs.json', 'w') as f:\n",
    "    json.dump(splits, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_data.close()\n",
    "prototype_data = tables.open_file(DATA_FOLDER + 'NIS_2012_2014_prototype_v2.h5', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset, subset_info in splits.items():\n",
    "    prototype_data.create_array(prototype_data.root, subset, np.array(subset_info['inds'], dtype='int32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/headers (Array(145,)) ''\n",
       "  atom := StringAtom(itemsize=14, shape=(), dflt=b'')\n",
       "  maindim := 0\n",
       "  flavor := 'python'\n",
       "  byteorder := 'irrelevant'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prototype_data.create_array(prototype_data.root, 'headers', headers_to_keep_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<closed File>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prototype_data_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_data_float.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/headers (Array(145,)) ''\n",
       "  atom := StringAtom(itemsize=11, shape=(), dflt=b'')\n",
       "  maindim := 0\n",
       "  flavor := 'python'\n",
       "  byteorder := 'irrelevant'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prototype_data = tables.open_file(DATA_FOLDER + 'NIS_2012_2014_prototype_v2_ccs.h5', 'r')\n",
    "prototype_data_float = tables.open_file(DATA_FOLDER + 'NIS_2012_2014_prototype_float_v2_ccs.h5', 'a')\n",
    "\n",
    "prototype_data_float.create_array('/', 'dataset', shape=prototype_data.root.dataset.shape, atom=tables.Float32Atom())\n",
    "prototype_data_float.create_array('/', 'TRAIN', prototype_data.root.TRAIN[:])\n",
    "prototype_data_float.create_array('/', 'VAL', prototype_data.root.VAL[:])\n",
    "prototype_data_float.create_array('/', 'TEST', prototype_data.root.TEST[:])\n",
    "prototype_data_float.create_array('/', 'headers', prototype_data.root.headers[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0 : 1000000\n",
      "Starting 1000000 : 2000000\n",
      "Starting 2000000 : 3000000\n",
      "Starting 3000000 : 4000000\n",
      "Starting 4000000 : 5000000\n",
      "Starting 5000000 : 6000000\n",
      "Starting 6000000 : 7000000\n",
      "Starting 7000000 : 8000000\n",
      "Starting 8000000 : 9000000\n",
      "Starting 9000000 : 10000000\n",
      "Starting 10000000 : 11000000\n",
      "Starting 11000000 : 12000000\n",
      "Starting 12000000 : 13000000\n",
      "Starting 13000000 : 14000000\n",
      "Starting 14000000 : 15000000\n",
      "Starting 15000000 : 16000000\n",
      "Starting 16000000 : 17000000\n",
      "Starting 17000000 : 18000000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "chunk_size = 1000000\n",
    "dataset = prototype_data.root.dataset\n",
    "\n",
    "while count < dataset.shape[0]:\n",
    "\n",
    "    print(\"Starting {0} : {1}\".format(count, count + chunk_size))\n",
    "    \n",
    "    # iteration\n",
    "    if count > dataset.shape[0]:\n",
    "        count = dataset.shape[0]\n",
    "    \n",
    "    # do something\n",
    "    chunk = dataset[count:count+chunk_size, :].astype('float32')\n",
    "    prototype_data_float.root.dataset[count:count+chunk_size, :] = chunk\n",
    "\n",
    "    # finalize iteration\n",
    "    count += chunk_size\n",
    "    if count == dataset.shape[0]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_data_float.close()\n",
    "prototype_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Mean, Unit Variance Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "import numpy as np\n",
    "import json\n",
    "from numba import jit\n",
    "\n",
    "DATA_FOLDER = '../data/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: prototype_data.close()\n",
    "except: pass\n",
    "\n",
    "prototype_data_float = tables.open_file(DATA_FOLDER + 'NIS_2012_2014_prototype_float_v2_ccs.h5', 'a')\n",
    "# prototype_data_safety = tables.open_file(DATA_FOLDER + 'safety/NIS_2012_2014_prototype.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = prototype_data_float.root.dataset[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def normalize(array):\n",
    "    mean, var = np.mean(array), np.var(array)\n",
    "    zero_mean = array - mean\n",
    "    unit_var = zero_mean / var\n",
    "    return unit_var, mean, var\n",
    "\n",
    "# warmup\n",
    "normalize(np.random.rand(10000).astype(np.float));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "objects of type ``CPUDispatcher`` are not supported in this context, sorry; supported objects are: NumPy array, record or scalar; homogeneous list or tuple, integer, float, complex or bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2a1f77e997ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprototype_data_float\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tables/array.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;31m# Create an array compliant with the specified slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         \u001b[0mnparr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_np_atom2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnparr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tables/utils.py\u001b[0m in \u001b[0;36mconvert_to_np_atom2\u001b[0;34m(object, atom)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;31m# safe to in-place conversion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'time64'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mnparr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_np_atom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0;31m# Finally, check the byteorder and change it if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mbyteorder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbyteorders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnparr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyteorder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tables/utils.py\u001b[0m in \u001b[0;36mconvert_to_np_atom\u001b[0;34m(arr, atom, copy)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# First, convert the object into a NumPy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mnparr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_of_flavor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;31m# Copy of data if necessary for getting a contiguous buffer, or if\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# dtype is not the correct one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tables/flavor.py\u001b[0m in \u001b[0;36marray_of_flavor\u001b[0;34m(array, dst_flavor)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \"\"\"\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_of_flavor2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflavor_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_flavor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tables/flavor.py\u001b[0m in \u001b[0;36mflavor_of\u001b[0;34m(array)\u001b[0m\n\u001b[1;32m    194\u001b[0m     raise TypeError(\n\u001b[1;32m    195\u001b[0m         \u001b[0;34m\"objects of type ``%s`` are not supported in this context, sorry; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \"supported objects are: %s\" % (type_name, supported_descs))\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: objects of type ``CPUDispatcher`` are not supported in this context, sorry; supported objects are: NumPy array, record or scalar; homogeneous list or tuple, integer, float, complex or bytes"
     ]
    }
   ],
   "source": [
    "prototype_data_float.root.dataset[:, 0] = normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_data_float.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucket Age Into Decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_data_embred = tables.open_file(DATA_FOLDER + 'NIS_2012_2014_prototype_float_v2_ccs.h5', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_normalized = prototype_data_embred.root.dataset[:1000, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'var' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-877d9001f6e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mage_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mage_normalized\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'var' is not defined"
     ]
    }
   ],
   "source": [
    "age_normalized = (age_normalized * var) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0 : 1000000\n",
      "Starting 1000000 : 2000000\n",
      "Starting 2000000 : 3000000\n",
      "Starting 3000000 : 4000000\n",
      "Starting 4000000 : 5000000\n",
      "Starting 5000000 : 6000000\n",
      "Starting 6000000 : 7000000\n",
      "Starting 7000000 : 8000000\n",
      "Starting 8000000 : 9000000\n",
      "Starting 9000000 : 10000000\n",
      "Starting 10000000 : 11000000\n",
      "Starting 11000000 : 12000000\n",
      "Starting 12000000 : 13000000\n",
      "Starting 13000000 : 14000000\n",
      "Starting 14000000 : 15000000\n",
      "Starting 15000000 : 16000000\n",
      "Starting 16000000 : 17000000\n",
      "Starting 17000000 : 18000000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "chunk_size = 1000000\n",
    "dataset = prototype_data_embred.root.dataset\n",
    "\n",
    "while count < dataset.shape[0]:\n",
    "\n",
    "    print(\"Starting {0} : {1}\".format(count, count + chunk_size))\n",
    "    \n",
    "    # iteration\n",
    "    if count > dataset.shape[0]:\n",
    "        count = dataset.shape[0]\n",
    "    \n",
    "    # do something\n",
    "    chunk_age = dataset[count:count+chunk_size, 0]\n",
    "    \n",
    "#     chunk_age *= var\n",
    "#     chunk_age += mean\n",
    "    \n",
    "    decade = np.floor(chunk_age / 10)\n",
    "    \n",
    "    prototype_data_embred.root.dataset[count:count+chunk_size, 0] = decade\n",
    "\n",
    "    # finalize iteration\n",
    "    count += chunk_size\n",
    "    if count == dataset.shape[0]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_decades = np.unique(prototype_data_embred.root.dataset[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_decades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_data_embred.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding Num Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NIS_cc_stroke_sgcs_psminput.csv',\n",
       " 'NIS_stroke_annoyindex_controls_100trees.ann',\n",
       " 'NIS_cc_stroke_psminput.csv',\n",
       " 'NIS_2014_Total.h5',\n",
       " 'safety',\n",
       " 'NIS_stroke_casecontrol_100trees.ann',\n",
       " 'archive',\n",
       " 'NIS_columnstats_2012_2014.json',\n",
       " 'NIS_cc_stroke_tableonedata.csv',\n",
       " 'NIS_2012_2014_prototype_float_v2.h5',\n",
       " 'psm_res',\n",
       " 'NIS_2012_Total.h5',\n",
       " 'NIS_2012_2014_proto_emb_v2.h5',\n",
       " 'NIS_stroke_casecontrol_orig.h5',\n",
       " 'Figure5_NumPVals.png',\n",
       " 'NIS_stroke_casecontrol.h5',\n",
       " 'NIS_controlindices.npy',\n",
       " 'NIS_2012_2014_prototype_float_v2_ccs.h5',\n",
       " 'NIS_2012_2014_proto_emb_v2_latentspace.h5',\n",
       " 'NIS_2012_2014_prototype_v2_ccs.h5',\n",
       " 'Figure4_PValueVisualization.png',\n",
       " 'NIS_2012_2014_prototype_TVTsplit_v2_ccs.json',\n",
       " 'NIS.h5',\n",
       " 'NIS_2012_2014_proto_emb_v2_anntree.h5',\n",
       " 'NIS_stroke_casecontrol_mod.h5',\n",
       " 'NIS_stroke_annoyindex_cases_100trees.ann',\n",
       " '.DS_Store',\n",
       " 'NIS_2013_Total.h5',\n",
       " 'NIS_Pruned.h5']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(DATA_FOLDER + 'NIS_columnstats_2012_2014.json', 'r') as f:\n",
    "    headers_found = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX1\n",
      "DX2\n",
      "DX3\n",
      "DX4\n",
      "DX5\n",
      "DX6\n",
      "DX7\n",
      "DX8\n",
      "DX9\n",
      "DX10\n",
      "DX11\n",
      "DX12\n",
      "DX13\n",
      "DX14\n",
      "DX15\n",
      "DX16\n",
      "DX17\n",
      "DX18\n",
      "DX19\n",
      "DX20\n",
      "DX21\n",
      "DX22\n",
      "DX23\n",
      "DX24\n",
      "DX25\n",
      "DX26\n",
      "DX27\n",
      "DX28\n",
      "DX29\n",
      "DX30\n",
      "Total unique DX codes: 12583\n"
     ]
    }
   ],
   "source": [
    "# Find the unique values\n",
    "unique_dx_codes_year = []\n",
    "for header, header_info in headers_found.items():\n",
    "    if header[:2] == 'DX' and header[2] in [str(i) for i in range(1, 10)]:\n",
    "        print(header)\n",
    "        for year_info in header_info.values():\n",
    "            unique_els = year_info['unique']\n",
    "            unique_dx_codes_year.extend(unique_els)\n",
    "\n",
    "unique_dx_codes = np.unique(unique_dx_codes_year)\n",
    "unique_dx_codes[unique_dx_codes == -128] = 0\n",
    "dx_mapping = {code : mapping for code, mapping in zip(unique_dx_codes, np.arange(0, len(unique_dx_codes)))}\n",
    "print(\"Total unique DX codes: {0}\".format(len(unique_dx_codes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR1\n",
      "PR2\n",
      "PR3\n",
      "PR4\n",
      "PR5\n",
      "PR6\n",
      "PR7\n",
      "PR8\n",
      "PR9\n",
      "PR10\n",
      "PR11\n",
      "PR12\n",
      "PR13\n",
      "PR14\n",
      "PR15\n",
      "Total unique PR codes: 4445\n"
     ]
    }
   ],
   "source": [
    "# Find the unique values\n",
    "unique_pr_codes_year = []\n",
    "for header, header_info in headers_found.items():\n",
    "    if header[:2] == 'PR' and header[2] in [str(i) for i in range(1, 10)]:\n",
    "        print(header)\n",
    "        for year_info in header_info.values():\n",
    "            unique_els = year_info['unique']\n",
    "            unique_pr_codes_year.extend(unique_els)\n",
    "\n",
    "unique_pr_codes = np.unique(unique_pr_codes_year)\n",
    "unique_pr_codes[unique_pr_codes == -128] = 0\n",
    "pr_mapping = {code : mapping for code, mapping in zip(unique_pr_codes, np.arange(0, len(unique_pr_codes)))}\n",
    "print(\"Total unique PR codes: {0}\".format(len(unique_pr_codes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECODE1\n",
      "ECODE2\n",
      "ECODE3\n",
      "ECODE4\n",
      "Total unique ECODES codes: 1186\n"
     ]
    }
   ],
   "source": [
    "# Find the unique values\n",
    "unique_ei_codes_year = []\n",
    "for header, header_info in headers_found.items():\n",
    "    if header[:5] == 'ECODE' and header[5] in [str(i) for i in range(1, 10)]:\n",
    "        print(header)\n",
    "        for year_info in header_info.values():\n",
    "            unique_els = year_info['unique']\n",
    "            unique_ei_codes_year.extend(unique_els)\n",
    "\n",
    "unique_ei_codes = np.unique(unique_ei_codes_year)\n",
    "unique_ei_codes[unique_ei_codes == -128] = 0\n",
    "ei_mapping = {code : mapping for code, mapping in zip(unique_ei_codes, np.arange(0, len(unique_ei_codes)))}\n",
    "print(\"Total unique ECODES codes: {0}\".format(len(unique_ei_codes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's map each of these codes onto unique indices and save this mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DXCCS1\n",
      "DXCCS2\n",
      "DXCCS3\n",
      "DXCCS4\n",
      "DXCCS5\n",
      "DXCCS6\n",
      "DXCCS7\n",
      "DXCCS8\n",
      "DXCCS9\n",
      "DXCCS10\n",
      "DXCCS11\n",
      "DXCCS12\n",
      "DXCCS13\n",
      "DXCCS14\n",
      "DXCCS15\n",
      "DXCCS16\n",
      "DXCCS17\n",
      "DXCCS18\n",
      "DXCCS19\n",
      "DXCCS20\n",
      "DXCCS21\n",
      "DXCCS22\n",
      "DXCCS23\n",
      "DXCCS24\n",
      "DXCCS25\n",
      "DXCCS26\n",
      "DXCCS27\n",
      "DXCCS28\n",
      "DXCCS29\n",
      "DXCCS30\n",
      "Total unique DXCCS codes: 263\n"
     ]
    }
   ],
   "source": [
    "# Find the unique values\n",
    "unique_ccs_codes_year = []\n",
    "for header, header_info in headers_found.items():\n",
    "    if header[:5] == 'DXCCS' and header[5] in [str(i) for i in range(1, 10)]:\n",
    "        print(header)\n",
    "        for year_info in header_info.values():\n",
    "            unique_els = year_info['unique']\n",
    "            unique_ccs_codes_year.extend(unique_els)\n",
    "\n",
    "unique_ccs_codes = np.unique(unique_ccs_codes_year)\n",
    "unique_ccs_codes[unique_ccs_codes == -128] = 0\n",
    "ccs_mapping = {code : mapping for code, mapping in zip(unique_ccs_codes, np.arange(0, len(unique_ccs_codes)))}\n",
    "print(\"Total unique DXCCS codes: {0}\".format(len(unique_ccs_codes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: prototype_data.close()\n",
    "except: pass\n",
    "\n",
    "prototype_data = tables.open_file(DATA_FOLDER + 'NIS_2012_2014_prototype_float_v2_ccs.h5', 'a')\n",
    "dataset = prototype_data.root.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_data_embmap = tables.open_file(DATA_FOLDER + 'NIS_2012_2014_proto_emb_v2_ccs.h5', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_data_embmap.create_array('/', 'dataset', shape=dataset.shape, atom=tables.Float32Atom())\n",
    "prototype_data_embmap.create_array('/', 'TRAIN', prototype_data.root.TRAIN[:])\n",
    "prototype_data_embmap.create_array('/', 'VAL', prototype_data.root.VAL[:])\n",
    "prototype_data_embmap.create_array('/', 'TEST', prototype_data.root.TEST[:])\n",
    "prototype_data_embmap.create_array('/', 'headers', prototype_data.root.headers[:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/mapping/DXCCSn (Array(263, 2)) ''\n",
       "  atom := Int64Atom(shape=(), dflt=0)\n",
       "  maindim := 0\n",
       "  flavor := 'numpy'\n",
       "  byteorder := 'little'\n",
       "  chunkshape := None"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx_map = np.vstack((np.array(list(dx_mapping.keys())), np.array(list(dx_mapping.values())))).T\n",
    "pr_map = np.vstack((np.array(list(pr_mapping.keys())), np.array(list(pr_mapping.values())))).T\n",
    "ei_map = np.vstack((np.array(list(ei_mapping.keys())), np.array(list(ei_mapping.values())))).T\n",
    "ccs_map = np.vstack((np.array(list(ccs_mapping.keys())), np.array(list(ccs_mapping.values())))).T\n",
    "\n",
    "prototype_data_embmap.create_group('/', 'mapping')\n",
    "prototype_data_embmap.create_array('/mapping', 'CHRONn', dx_map)\n",
    "prototype_data_embmap.create_array('/mapping', 'DXn', dx_map)\n",
    "prototype_data_embmap.create_array('/mapping', 'PRn', pr_map)\n",
    "prototype_data_embmap.create_array('/mapping', 'ECODEn', ei_map)\n",
    "prototype_data_embmap.create_array('/mapping', 'DXCCSn', ccs_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_feature(pattern, headers):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    idxs = []\n",
    "    for idx, header in enumerate(headers):\n",
    "        header = header.decode('utf-8')\n",
    "        lp = len(pattern)\n",
    "        if header[:lp] == pattern and header[lp] in [str(i) for i in range(0, 10)]:\n",
    "            idxs.append(idx)\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "chron_idxs = find_feature('CHRON', prototype_data_embmap.root.headers[:])\n",
    "dx_idxs = find_feature('DX', prototype_data_embmap.root.headers[:])\n",
    "pr_idxs = find_feature('PR', prototype_data_embmap.root.headers[:])\n",
    "ei_idxs = find_feature('ECODE', prototype_data_embmap.root.headers[:])\n",
    "ccs_idxs = find_feature('DXCCS', prototype_data_embmap.root.headers[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_code_to_map(array, idxs, mapping):\n",
    "    for idx in idxs:\n",
    "        array[:, idx] = mapping[array[:, idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0 : 1000000\n",
      "Starting 1000000 : 2000000\n",
      "Starting 2000000 : 3000000\n",
      "Starting 3000000 : 4000000\n",
      "Starting 4000000 : 5000000\n",
      "Starting 5000000 : 6000000\n",
      "Starting 6000000 : 7000000\n",
      "Starting 7000000 : 8000000\n",
      "Starting 8000000 : 9000000\n",
      "Starting 9000000 : 10000000\n",
      "Starting 10000000 : 11000000\n",
      "Starting 11000000 : 12000000\n",
      "Starting 12000000 : 13000000\n",
      "Starting 13000000 : 14000000\n",
      "Starting 14000000 : 15000000\n",
      "Starting 15000000 : 16000000\n",
      "Starting 16000000 : 17000000\n",
      "Starting 17000000 : 18000000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "chunk_size = 1000000\n",
    "dataset = prototype_data.root.dataset\n",
    "\n",
    "while count < dataset.shape[0]:\n",
    "\n",
    "    print(\"Starting {0} : {1}\".format(count, count + chunk_size))\n",
    "    \n",
    "    # iteration\n",
    "    if count > dataset.shape[0]:\n",
    "        count = dataset.shape[0]\n",
    "    \n",
    "    # do something\n",
    "    chunk = dataset[count:count+chunk_size, :].astype('float32')\n",
    "    prototype_data_embmap.root.dataset[count:count+chunk_size, :] = chunk\n",
    "\n",
    "    # finalize iteration\n",
    "    count += chunk_size\n",
    "    if count == dataset.shape[0]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdx = np.array(list(dx_mapping.keys()))\n",
    "vdx = np.array(list(dx_mapping.values()))\n",
    "\n",
    "mapping_dx = np.zeros(kdx.max()+1, dtype=vdx.dtype) #k,v from approach #1\n",
    "mapping_dx[kdx] = vdx\n",
    "\n",
    "kpr = np.array(list(pr_mapping.keys()))\n",
    "vpr = np.array(list(pr_mapping.values()))\n",
    "\n",
    "mapping_pr = np.zeros(kpr.max()+1, dtype=vpr.dtype) #k,v from approach #1\n",
    "mapping_pr[kpr] = vpr\n",
    "\n",
    "kei = np.array(list(ei_mapping.keys()))\n",
    "vei = np.array(list(ei_mapping.values()))\n",
    "\n",
    "mapping_ei = np.zeros(kei.max()+1, dtype=vei.dtype) #k,v from approach #1\n",
    "mapping_ei[kei] = vei\n",
    "\n",
    "kci = np.array(list(ccs_mapping.keys()))\n",
    "vci = np.array(list(ccs_mapping.values()))\n",
    "\n",
    "mapping_ccs = np.zeros(kci.max()+1, dtype=vci.dtype) #k,v from approach #1\n",
    "mapping_ccs[kci] = vci\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0 : 1000000\n",
      "Starting 1000000 : 2000000\n",
      "Starting 2000000 : 3000000\n",
      "Starting 3000000 : 4000000\n",
      "Starting 4000000 : 5000000\n",
      "Starting 5000000 : 6000000\n",
      "Starting 6000000 : 7000000\n",
      "Starting 7000000 : 8000000\n",
      "Starting 8000000 : 9000000\n",
      "Starting 9000000 : 10000000\n",
      "Starting 10000000 : 11000000\n",
      "Starting 11000000 : 12000000\n",
      "Starting 12000000 : 13000000\n",
      "Starting 13000000 : 14000000\n",
      "Starting 14000000 : 15000000\n",
      "Starting 15000000 : 16000000\n",
      "Starting 16000000 : 17000000\n",
      "Starting 17000000 : 18000000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "chunk_size = 1000000\n",
    "dataset = prototype_data.root.dataset\n",
    "\n",
    "\n",
    "# out = mapping_ar[input_array]\n",
    "\n",
    "while count < dataset.shape[0]:\n",
    "\n",
    "    print(\"Starting {0} : {1}\".format(count, count + chunk_size))\n",
    "    \n",
    "    # iteration\n",
    "    if count > dataset.shape[0]:\n",
    "        count = dataset.shape[0]\n",
    "    \n",
    "    # do something\n",
    "    chunk = dataset[count:count+chunk_size, :]\n",
    "    \n",
    "    chunk[:, chron_idxs] = mapping_dx[chunk[:, chron_idxs].ravel().astype('int64')].reshape(-1, len(chron_idxs))\n",
    "    chunk[:, dx_idxs] = mapping_dx[chunk[:, dx_idxs].ravel().astype('int64')].reshape(-1, len(dx_idxs))\n",
    "    chunk[:, pr_idxs] = mapping_pr[chunk[:, pr_idxs].ravel().astype('int64')].reshape(-1, len(pr_idxs))\n",
    "    chunk[:, ei_idxs] = mapping_ei[chunk[:, ei_idxs].ravel().astype('int64')].reshape(-1, len(ei_idxs))\n",
    "    chunk[:, ccs_idxs] = mapping_ccs[chunk[:, ccs_idxs].ravel().astype('int64')].reshape(-1, len(ccs_idxs))\n",
    "    \n",
    "    prototype_data_embmap.root.dataset[count:count+chunk_size, :] = chunk\n",
    "\n",
    "    # finalize iteration\n",
    "    count += chunk_size\n",
    "    if count == dataset.shape[0]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prototype_data_embmap.root.dataset[:, ccs_idxs].ravel().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_data_embmap.close()\n",
    "prototype_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing Dimension Space for DX Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "NodeError",
     "evalue": "group ``/`` already has a child node named ``dataset``",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNodeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-b18b96b3c400>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprototype_data_embmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_FOLDER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'NIS_2012_2014_proto_emb_red_v2.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprototype_data_embmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloat32Atom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprototype_data_embmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TRAIN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprototype_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprototype_data_embmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'VAL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprototype_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tables/file.py\u001b[0m in \u001b[0;36mcreate_array\u001b[0;34m(self, where, name, obj, title, byteorder, createparents, atom, shape, track_times)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         return Array(parentnode, name,\n\u001b[1;32m   1158\u001b[0m                      \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyteorder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbyteorder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                      track_times=track_times)\n\u001b[0m\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tables/array.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parentnode, name, obj, title, byteorder, _log, _atom, track_times)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# Ordinary arrays have no filters: leaf is created with default ones.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         super(Array, self).__init__(parentnode, name, new, Filters(),\n\u001b[0;32m--> 193\u001b[0;31m                                     byteorder, _log, track_times)\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_g_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tables/leaf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parentnode, name, new, filters, byteorder, _log, track_times)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLeaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparentnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tables/node.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parentnode, name, _log)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;31m# Only new nodes need to be referenced.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# Opened nodes are already known by their parent group.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mparentnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_refnode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_set_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparentnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tables/group.py\u001b[0m in \u001b[0;36m_g_refnode\u001b[0;34m(self, childnode, childname, validate)\u001b[0m\n\u001b[1;32m    524\u001b[0m             raise NodeError(\n\u001b[1;32m    525\u001b[0m                 \u001b[0;34m\"group ``%s`` already has a child node named ``%s``\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m                 % (self._v_pathname, childname))\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;31m# Show a warning if there is an object attribute with that name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNodeError\u001b[0m: group ``/`` already has a child node named ``dataset``"
     ]
    }
   ],
   "source": [
    "prototype_data = tables.open_file(DATA_FOLDER + 'NIS_2012_2014_prototype_float_v2.h5', 'a')\n",
    "prototype_data_embmap = tables.open_file(DATA_FOLDER + 'NIS_2012_2014_proto_emb_red_v2.h5', 'a')\n",
    "\n",
    "prototype_data_embmap.create_array('/', 'dataset', shape=dataset.shape, atom=tables.Float32Atom())\n",
    "prototype_data_embmap.create_array('/', 'TRAIN', prototype_data.root.TRAIN[:])\n",
    "prototype_data_embmap.create_array('/', 'VAL', prototype_data.root.VAL[:])\n",
    "prototype_data_embmap.create_array('/', 'TEST', prototype_data.root.TEST[:])\n",
    "prototype_data_embmap.create_array('/', 'headers', prototype_data.root.headers[:])\n",
    "\n",
    "prototype_data_embmap.create_group('/', 'mapping')\n",
    "prototype_data_embmap.create_array('/mapping', 'CHRONn', dx_map)\n",
    "prototype_data_embmap.create_array('/mapping', 'DXn', dx_map)\n",
    "prototype_data_embmap.create_array('/mapping', 'PRn', pr_map)\n",
    "prototype_data_embmap.create_array('/mapping', 'ECODEn', ei_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISS_VAL_FILL = 0\n",
    "\n",
    "@jit(nopython=True)\n",
    "def reduce_icd9_complexity(array, depth=1):\n",
    "    \"\"\"Map numerical rep of ICD9 code to ICD9 code\"\"\"\n",
    "#     cs = str(int(code))\n",
    "#     len_code = len(cs)\n",
    "    \n",
    "#     if code >= 200000:\n",
    "#         # external injury code (E000.0-E999.9) => (E000 - E990)\n",
    "#         b = 200000\n",
    "#         scale = 10 ** (5 - len_code)\n",
    "        \n",
    "#     elif code >= 100000:\n",
    "#         # supplementary code (V01.00-V99.99) => (V01 - V99)\n",
    "#         b = 100000\n",
    "#         scale = 10 ** (4 - len_code)\n",
    "        \n",
    "#     elif code < 100000:\n",
    "#         # regular ICD9 code (000.00-999.99) => (000 - 999)\n",
    "#         b = 0\n",
    "#         scale = 10 ** (5 - len_code)\n",
    "        \n",
    "#     else:\n",
    "#         raise ValueError(\"Something went wrong with {0}\".format(code))\n",
    "        \n",
    "#     original_code = (str(code) - b) / scale\n",
    "#     print(original_code)\n",
    "#     new_code = str(original_code)[:-depth]\n",
    "    return np.floor(array / 10**(depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's reduce the numerical ICD9 codes to the desired depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0 : 1000000\n",
      "Starting 1000000 : 2000000\n",
      "Starting 2000000 : 3000000\n",
      "Starting 3000000 : 4000000\n",
      "Starting 4000000 : 5000000\n",
      "Starting 5000000 : 6000000\n",
      "Starting 6000000 : 7000000\n",
      "Starting 7000000 : 8000000\n",
      "Starting 8000000 : 9000000\n",
      "Starting 9000000 : 10000000\n",
      "Starting 10000000 : 11000000\n",
      "Starting 11000000 : 12000000\n",
      "Starting 12000000 : 13000000\n",
      "Starting 13000000 : 14000000\n",
      "Starting 14000000 : 15000000\n",
      "Starting 15000000 : 16000000\n",
      "Starting 16000000 : 17000000\n",
      "Starting 17000000 : 18000000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "chunk_size = 1000000\n",
    "dataset = prototype_data.root.dataset\n",
    "\n",
    "while count < dataset.shape[0]:\n",
    "\n",
    "    print(\"Starting {0} : {1}\".format(count, count + chunk_size))\n",
    "    \n",
    "    # iteration\n",
    "    if count > dataset.shape[0]:\n",
    "        count = dataset.shape[0]\n",
    "    \n",
    "    # do something\n",
    "    chunk = dataset[count:count+chunk_size, :]\n",
    "    \n",
    "    chunk[:, chron_idxs] = reduce_icd9_complexity(chunk[:, chron_idxs].ravel()).reshape(-1, len(chron_idxs))\n",
    "    chunk[:, dx_idxs] = reduce_icd9_complexity(chunk[:, dx_idxs].ravel()).reshape(-1, len(dx_idxs))\n",
    "    chunk[:, pr_idxs] = reduce_icd9_complexity(chunk[:, pr_idxs].ravel()).reshape(-1, len(pr_idxs))\n",
    "    chunk[:, ei_idxs] = reduce_icd9_complexity(chunk[:, ei_idxs].ravel()).reshape(-1, len(ei_idxs))\n",
    "    \n",
    "    prototype_data_embmap.root.dataset[count:count+chunk_size, :] = chunk\n",
    "\n",
    "    # finalize iteration\n",
    "    count += chunk_size\n",
    "    if count == dataset.shape[0]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10919.0, 10881.0)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prototype_data_embmap.root.dataset[:, dx_idxs].ravel().max(), prototype_data_embmap.root.dataset[:, chron_idxs].ravel().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_chunk_codes_reduced_by_depth(chunk, patterns, depth):\n",
    "    dx_maps = {}\n",
    "    # Find unique codes first\n",
    "    for pattern in patterns:\n",
    "        \n",
    "        if pattern == 'CHRON':\n",
    "            search_pattern = 'DX'\n",
    "        else:\n",
    "            search_pattern = pattern\n",
    "        \n",
    "        lp = len(search_pattern)\n",
    "        unique_dx_codes_year = []\n",
    "        for header, header_info in headers_found.items():\n",
    "            if header[:lp] == search_pattern and header[lp] in [str(i) for i in range(1, 10)]:\n",
    "    #             print(header)\n",
    "                for year_info in header_info.values():\n",
    "                    unique_els = year_info['unique']\n",
    "                    unique_dx_codes_year.extend(unique_els)\n",
    "        \n",
    "                    \n",
    "        unique_dx_codes = np.unique(unique_dx_codes_year)\n",
    "        unique_dx_codes[unique_dx_codes == -128] = 0\n",
    "\n",
    "        unique_dx_codes = np.unique(reduce_icd9_complexity(unique_dx_codes, depth)).astype('int64')\n",
    "\n",
    "        dx_mapping = {code : mapping for code, mapping in zip(unique_dx_codes, np.arange(0, len(unique_dx_codes)))}\n",
    "        print(\"Total unique {1} codes: {0}\".format(len(unique_dx_codes), pattern))\n",
    "\n",
    "        dx_idxs = find_feature(pattern, prototype_data_embmap.root.headers[:])\n",
    "\n",
    "        dx_maps[pattern] = np.vstack((np.array(list(dx_mapping.keys())), np.array(list(dx_mapping.values())))).T\n",
    "\n",
    "        kdx = np.array(list(dx_mapping.keys()))\n",
    "        vdx = np.array(list(dx_mapping.values()))\n",
    "\n",
    "        mapping_dx = np.zeros(kdx.max()+1, dtype=vdx.dtype) #k,v from approach #1\n",
    "        mapping_dx[kdx] = vdx\n",
    "        \n",
    "        chunk[:, dx_idxs] = mapping_dx[chunk[:, dx_idxs].ravel().astype('int64')].reshape(-1, len(dx_idxs))\n",
    "    \n",
    "    return chunk, dx_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0 : 1000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 1000000 : 2000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 2000000 : 3000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 3000000 : 4000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 4000000 : 5000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 5000000 : 6000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 6000000 : 7000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 7000000 : 8000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 8000000 : 9000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 9000000 : 10000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 10000000 : 11000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 11000000 : 12000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 12000000 : 13000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 13000000 : 14000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 14000000 : 15000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 15000000 : 16000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 16000000 : 17000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n",
      "Starting 17000000 : 18000000\n",
      "Total unique CHRON codes: 6202\n",
      "Total unique DX codes: 6202\n",
      "Total unique PR codes: 3722\n",
      "Total unique ECODE codes: 182\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "chunk_size = 1000000\n",
    "dataset = prototype_data_embmap.root.dataset\n",
    "\n",
    "while count < dataset.shape[0]:\n",
    "\n",
    "    print(\"Starting {0} : {1}\".format(count, count + chunk_size))\n",
    "    \n",
    "    # iteration\n",
    "    if count > dataset.shape[0]:\n",
    "        count = dataset.shape[0]\n",
    "    \n",
    "    # do something\n",
    "    chunk = dataset[count:count+chunk_size, :]\n",
    "    \n",
    "    chunk, maps = map_chunk_codes_reduced_by_depth(chunk, ['CHRON', 'DX', 'PR', 'ECODE'], 1)\n",
    "    \n",
    "    if count == 0:\n",
    "        for k, kmap in maps.items():\n",
    "            setattr(prototype_data_embmap.root.mapping, ('{0}n'.format(k)), kmap)\n",
    "    \n",
    "    prototype_data_embmap.root.dataset[count:count+chunk_size, :] = chunk\n",
    "\n",
    "    # finalize iteration\n",
    "    count += chunk_size\n",
    "    if count == dataset.shape[0]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6201.0, 6189.0)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prototype_data_embmap.root.dataset[:, dx_idxs].ravel().max(), prototype_data_embmap.root.dataset[:, chron_idxs].ravel().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_data.close()\n",
    "prototype_data_embmap.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
